#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import argparse
import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from tqdm import tqdm

def load_model(model_path="./my_bert_model"):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨"""
    tokenizer = BertTokenizer.from_pretrained(model_path)
    model = BertForSequenceClassification.from_pretrained(model_path)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device).eval()
    return model, tokenizer, device

def predict_batch(texts, model, tokenizer, device, batch_size=32, max_length=256):
    """æ‰¹é‡é¢„æµ‹æ–‡æœ¬åˆ—è¡¨"""
    results = []
    for i in tqdm(range(0, len(texts), batch_size), desc="é¢„æµ‹è¿›åº¦"):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(
            batch_texts,
            truncation=True,
            padding="max_length",
            max_length=max_length,
            return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        pred_indices = torch.argmax(probs, dim=1).cpu().numpy()
        
        for idx in pred_indices:
            # ç›´æ¥è·å–æ ‡ç­¾åç§°ï¼Œä¸ä¿ç•™ç½®ä¿¡åº¦å’ŒID
            results.append(model.config.id2label[idx])
    return results

if __name__ == "__main__":
    # å‚æ•°è§£æï¼ˆé»˜è®¤è·¯å¾„è®¾ç½®ï¼‰
    parser = argparse.ArgumentParser(description="BERTæ–‡æœ¬åˆ†ç±»æ‰¹é‡é¢„æµ‹")
    parser.add_argument("--input", type=str, default="./dataset/test_text.csv", 
                        help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šdataset/test_text.csvï¼‰")
    parser.add_argument("--output", type=str, default="./dataset/submit.csv", 
                        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šsubmit.csvï¼‰")
    parser.add_argument("--model", type=str, default="./my_bert_model", 
                        help="æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ï¼š./my_bert_modelï¼‰")
    args = parser.parse_args()

    # 1. åŠ è½½æ¨¡å‹
    model, tokenizer, device = load_model(args.model)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | è®¾å¤‡: {device}")
    print("ğŸ” æ ‡ç­¾æ˜ å°„:", model.config.id2label)

    # 2. è¯»å–æ•°æ®ï¼ˆå¤„ç†ä¸‰åˆ—æ ¼å¼ï¼‰
    try:
        # è¯»å–åŒ…å«ä¸‰åˆ—ï¼ˆIDã€ç±»åˆ«ã€æ–‡æœ¬ï¼‰çš„CSVæ–‡ä»¶
        df = pd.read_csv(args.input)
        
        # éªŒè¯å¿…é¡»åŒ…å«çš„åˆ—
        required_columns = ["id", "æ–‡æœ¬"]
        if not set(required_columns).issubset(df.columns):
            missing = set(required_columns) - set(df.columns)
            raise KeyError(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—: {missing}")
        
        texts = df["æ–‡æœ¬"].tolist()
        ids = df["id"].tolist()
        print(f"ğŸ“¥ åŠ è½½æ•°æ®: {args.input} | æ ·æœ¬æ•°: {len(texts)}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        exit(1)

    # 3. æ‰¹é‡é¢„æµ‹
    predicted_labels = predict_batch(texts, model, tokenizer, device)

    # 4. ä¿å­˜ç»“æœï¼ˆä»…ä¿ç•™IDå’Œæ–‡æœ¬ç±»åˆ«ä¸¤åˆ—ï¼‰
    result_df = pd.DataFrame({
        "id": ids,
        "ç±»åˆ«": predicted_labels
    })
    result_df.to_csv(args.output, index=False)
    
    # 5. ç»Ÿè®¡æŠ¥å‘Š
    label_counts = result_df["ç±»åˆ«"].value_counts()
    print(f"\nâœ… é¢„æµ‹å®Œæˆ! ç»“æœä¿å­˜è‡³: {args.output}")
    print("ğŸ“Š ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:")
    print(label_counts.to_string())